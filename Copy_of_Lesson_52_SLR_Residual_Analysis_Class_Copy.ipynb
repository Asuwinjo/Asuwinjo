{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Lesson 52: SLR - Residual Analysis - Class Copy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Asuwinjo/Asuwinjo/blob/main/Copy_of_Lesson_52_SLR_Residual_Analysis_Class_Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZI4n_qziUBb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctPdsMcc5QzN"
      },
      "source": [
        "# Run the code cell.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Loading the dataset.\n",
        "csv_file = 'https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/air-quality/AirQualityUCI.csv'\n",
        "df = pd.read_csv(csv_file, sep=';')\n",
        "\n",
        "# Dropping the 'Unnamed: 15' & 'Unnamed: 16' columns.\n",
        "df = df.drop(columns=['Unnamed: 15', 'Unnamed: 16'], axis=1) \n",
        "\n",
        "# Dropping the null values.\n",
        "df = df.dropna()\n",
        "\n",
        "# Creating a Pandas series containing 'datetime' objects.\n",
        "dt_series = pd.Series(data = [item.split(\"/\")[2] + \"-\" + item.split(\"/\")[1] + \"-\" + item.split(\"/\")[0] for item in df['Date']], index=df.index) + ' ' + pd.Series(data=[str(item).replace(\".\", \":\") for item in df['Time']], index=df.index)\n",
        "dt_series = pd.to_datetime(dt_series)\n",
        "\n",
        "# Remove the Date & Time columns from the DataFrame and insert the 'dt_series' in it.\n",
        "df = df.drop(columns=['Date', 'Time'], axis=1)\n",
        "df.insert(loc=0, column='DateTime', value=dt_series)\n",
        "\n",
        "# Get the Pandas series containing the year values as integers.\n",
        "year_series = dt_series.dt.year\n",
        "\n",
        "# Get the Pandas series containing the month values as integers.\n",
        "month_series = dt_series.dt.month\n",
        "\n",
        "# Get the Pandas series containing the day values as integers.\n",
        "day_series = dt_series.dt.day\n",
        "\n",
        "# Get the Pandas series containing the days of a week, i.e., Monday, Tuesday, Wednesday etc.\n",
        "day_name_series = dt_series.dt.day_name()\n",
        "\n",
        "# Add the 'Year', 'Month', 'Day' and 'Day Name' columns to the DataFrame.\n",
        "df['Year'] = year_series\n",
        "df['Month'] = month_series\n",
        "df['Day'] = day_series\n",
        "df['Day Name'] = day_name_series\n",
        "\n",
        "# Sort the DataFrame by the 'DateTime' values in the ascending order. Also, display the first 10 rows of the DataFrame.\n",
        "df = df.sort_values(by='DateTime')\n",
        "\n",
        "# Create a function to replace the commas with periods in a Pandas series.\n",
        "def comma_to_period(series):\n",
        "    new_series = pd.Series(data=[float(str(item).replace(',', '.')) for item in series], index=df.index)\n",
        "    return new_series\n",
        "\n",
        "# Apply the 'comma_to_period()' function on the ''CO(GT)', 'C6H6(GT)', 'T', 'RH' and 'AH' columns.\n",
        "cols_to_correct = ['CO(GT)', 'C6H6(GT)', 'T', 'RH', 'AH'] # Create a list of column names.\n",
        "for col in cols_to_correct: # Iterate through each column\n",
        "    df[col] = comma_to_period(df[col]) # Replace the original column with the new series.\n",
        "\n",
        "# Remove all the columns from the 'df' DataFrame containing more than 10% garbage value.\n",
        "df = df.drop(columns=['NMHC(GT)', 'CO(GT)', 'NOx(GT)', 'NO2(GT)'], axis=1)\n",
        "\n",
        "# Create a new DataFrame containing records for the years 2004 and 2005.\n",
        "aq_2004_df = df[df['Year'] == 2004]\n",
        "aq_2005_df = df[df['Year'] == 2005]\n",
        "\n",
        "# Replace the -200 value with the median values for each column having indices between 1 and -4 (excluding -4) for the 2004 year DataFrame.\n",
        "for col in aq_2004_df.columns[1:-4]:\n",
        "  median = aq_2004_df.loc[aq_2004_df[col] != -200, col].median()\n",
        "  aq_2004_df[col] = aq_2004_df[col].replace(to_replace=-200, value=median)\n",
        "\n",
        "# Repeat the same exercise for the 2005 year DataFrame.\n",
        "for col in aq_2005_df.columns[1:-4]:\n",
        "  median = aq_2005_df.loc[aq_2005_df[col] != -200, col].median()\n",
        "  aq_2005_df[col] = aq_2005_df[col].replace(to_replace=-200, value=median)\n",
        "\n",
        "# Group the DataFrames about the 'Month' column.\n",
        "group_2004_month = aq_2004_df.groupby(by='Month')\n",
        "group_2005_month = aq_2005_df.groupby(by='Month')\n",
        "\n",
        "# Concatenate the two DataFrames for 2004 and 2005 to obtain one DataFrame.\n",
        "df = pd.concat([aq_2004_df, aq_2005_df])\n",
        "\n",
        "# Information of the DataFrame.\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kRDMaMHAfLv"
      },
      "source": [
        "# Heatmap to pinpoint the columns in the 'df' DataFrame exhibiting high correlation.\n",
        "corr_df = df.iloc[:, 1:-4].corr()\n",
        "plt.figure(figsize = (10, 6), dpi = 96)\n",
        "sns.heatmap(data = corr_df, annot = True) # 'annot=True' fills the R values in the heatmap cells.\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oJqc-KisKSS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYoe69GREXWK"
      },
      "source": [
        "# Splitting the DataFrame into the train and test sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['T'] # Pandas DataFrame containing only feature variables\n",
        "y = df['RH'] # Pandas Series containing the target variable\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42) # Test set will have 33% of the values.\n",
        "\n",
        "# Create the 'errors_product()' and 'squared_errors()' function.\n",
        "def errors_product():\n",
        "  prod = (X_train - X_train.mean()) * (y_train - y_train.mean())\n",
        "  return prod\n",
        "\n",
        "def squared_errors():\n",
        "  sq_errors = (X_train - X_train.mean()) ** 2\n",
        "  return sq_errors\n",
        "\n",
        "# Calculate the slope and intercept values for the best fit line.\n",
        "slope = errors_product().sum()/ squared_errors().sum() \n",
        "intercept = y_train.mean() - slope * X_train.mean()\n",
        "\n",
        "print(f\"Slope: {slope} \\nIntercept: {intercept}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxk__XRLarjI"
      },
      "source": [
        "# Plot the regression line in the scatter plot between relative humidity and temperature values.\n",
        "plt.figure(figsize = (12, 5), dpi = 96)\n",
        "plt.title(\"Regression Line\", fontsize = 16)\n",
        "plt.scatter(df['T'], df['RH'])\n",
        "plt.plot(df['T'], slope * df['T'] + intercept, color = 'r', linewidth = 2, label = '$y = -1.1120x + 69.6911$')\n",
        "plt.xlabel(\"Temperature\")\n",
        "plt.ylabel(\"Relative Humidity\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksDGp9bxL-P1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymy1O0E5Kg7F"
      },
      "source": [
        "# Deploy linear regression model using the 'sklearn.linear_model' module.\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X_train_reshaped = X_train.values.reshape(-1, 1)\n",
        "y_train_reshaped = y_train.values.reshape(-1, 1)\n",
        "X_test_reshaped = X_test.values.reshape(-1, 1)\n",
        "y_test_reshaped = y_test.values.reshape(-1, 1)\n",
        "\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train_reshaped, y_train_reshaped)\n",
        "\n",
        "print(\"Coefficient of $x$ (or slope) ==>\", lin_reg.coef_)\n",
        "print(\"Intercept ==>\", lin_reg.intercept_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjNtJsmHLJLK"
      },
      "source": [
        "# Evaluate the linear regression model using the 'r2_score', 'mean_squared_error' & 'mean_absolute_error' functions of the 'sklearn' module.\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "y_train_pred = lin_reg.predict(X_train_reshaped)\n",
        "y_test_pred = lin_reg.predict(X_test_reshaped)\n",
        "\n",
        "print(f\"Train Set\\n{'-' * 50}\")\n",
        "print(f\"R-squared: {r2_score(y_train_reshaped, y_train_pred):.3f}\")\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_train_reshaped, y_train_pred):.3f}\")\n",
        "print(f\"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_train_reshaped, y_train_pred)):.3f}\")\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error(y_train_reshaped, y_train_pred):.3f}\")\n",
        "      \n",
        "print(f\"\\n\\nTest Set\\n{'-' * 50}\")\n",
        "print(f\"R-squared: {r2_score(y_test_reshaped, y_test_pred):.3f}\")\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_test_reshaped, y_test_pred):.3f}\")\n",
        "print(f\"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test_reshaped, y_test_pred)):.3f}\")\n",
        "print(f\"Mean Absolute Error: {mean_absolute_error(y_test_reshaped, y_test_pred):.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfFDBNMDM87Z"
      },
      "source": [
        "# S1.1: Create a histogram for the errors obtained in the predicted values for the train set.\n",
        "errors_train = y_train_reshaped - y_train_pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNAzm7qbdVpE"
      },
      "source": [
        "# S1.2: Create a normal distribution curve for the errors obtained in the predicted values for the train set.\n",
        "def prob_density_func(series):\n",
        "  CONST = 1 / (series.std() * np.sqrt(2 * np.pi))\n",
        "  power_of_e = - (series - series.mean()) ** 2 / (2 * series.var()) # 'pd.Series.var()' function returns the variance of the series.\n",
        "  new_array = CONST * np.exp(power_of_e)\n",
        "  return new_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjrTjO2U2NBf"
      },
      "source": [
        "---"
      ]
    }
  ]
}